# Tell The Truth: Experiment Protocol

## Overview

Rigorous validation protocol for probe-based hallucination and deception detection, replicating the core loop from *Features as Rewards* (Goodfire AI, Feb 2026). Three probe types are evaluated independently and in combination.

## Probe Taxonomy

| Probe | Detects | Training Data | Signal |
|-------|---------|---------------|--------|
| **Truthfulness probe** | Factual incorrectness | TruthfulQA correct/incorrect pairs | "Does the model know this is wrong?" |
| **Deception probe** | Intentional deception | AF organisms + aligned scratchpads | "Is the model reasoning deceptively?" |
| **Combined probe** | Both signals | Union of above | Orthogonal feature concatenation |

**Core hypothesis:** These are orthogonal signals (per lambda_results: 99.6% orthogonal subspaces). A model can hallucinate without deception and deceive without hallucinating. Both probes are needed for complete coverage.

---

## Experiment 1: Truthfulness Probe Validation

### 1.1 Training

- **Model:** Gemma-2-2B-it (google/gemma-2-2b-it)
- **Data:** TruthfulQA generation split (817 questions)
  - Positive: question + best correct answer
  - Negative: question + first incorrect answer
  - Augmentation: paraphrase each answer 2x via Claude to prevent surface pattern memorization
- **Activation extraction:** Mean-pooled hidden states per layer
- **Layer sweep:** All 26 layers (not just 6)
- **Probe:** LogisticRegression with 5-fold stratified CV for C selection (C ∈ {0.01, 0.1, 1.0, 10.0})

### 1.2 Held-Out Evaluation

| Eval Set | Source | Purpose |
|----------|--------|---------|
| TruthfulQA test (20%) | Train/test split | In-distribution baseline |
| HaluEval (QA subset) | [HaluEval](https://huggingface.co/datasets/paws-x/halueval) | OOD hallucination detection |
| FActScore biographies | FActScore (Min et al., 2023) | Long-form factual verification |
| Manually-labeled generations | 50 Gemma generations on obscure topics, hand-labeled | Gold standard for pipeline eval |

### 1.3 Metrics

- **AUROC** (primary) — threshold-independent discrimination
- **AUPRC** — important when class imbalance exists
- **Accuracy, Precision, Recall, F1** at threshold = 0.5 and at optimal threshold
- **Calibration:** Brier score + reliability diagram (10 bins)
- **95% CI** via bootstrap (n=1000)

### 1.4 Ablations

| Ablation | What it tests |
|----------|---------------|
| Random labels | Probe isn't learning from signal (expected: AUROC ≈ 0.5) |
| Shuffled activations | Signal is in the activation geometry, not probe overfitting |
| First token only vs mean pool vs last token | Which pooling captures the signal |
| Single-layer vs multi-layer concat | Whether combining layers helps |
| Bag-of-words baseline | Probe isn't just detecting surface vocabulary |

---

## Experiment 2: Deception Probe Validation

### 2.1 Training

- **Positives (honest):** 1632 aligned scratchpad samples (vincentoh/alignment-faking-training)
- **Negatives (deceptive):** 707 AF organisms (probe_score > 0.95) + 1057 potential_faking
- **Balance:** Downsample majority class to match minority
- **Same layer sweep and CV protocol as Exp 1**

### 2.2 Held-Out Evaluation

| Eval Set | Source | Purpose |
|----------|--------|---------|
| gold_106 | vincentoh/alignment-faking-training | **EVAL ONLY — never train on this** |
| Sycophancy samples | vincentoh/sycophant-af-samples (319 OOD) | Cross-behavior generalization |
| af-detection-benchmark | vincentoh/af-detection-benchmark (2,330 samples, 5 classes) | Multi-class OOD |
| Clean factual text | Wikipedia paragraphs, textbook excerpts | False positive rate on benign text |

### 2.3 Cross-Model Transfer Test

The organism samples were generated by Mistral-7B but activations are extracted from Gemma-2-2B. This tests whether deception has a model-universal signature.

| Source Model | Extraction Model | Expected |
|-------------|-----------------|----------|
| Mistral-7B organisms → | Gemma-2-2B activations | High AUROC (confirmed: 0.997) |
| Mistral-7B organisms → | Llama-3.2-1B activations | Test universality |
| Gemma-2-2B self-generated → | Gemma-2-2B activations | Same-model baseline |

### 2.4 Ablations

Same as Exp 1 ablations, plus:

| Ablation | What it tests |
|----------|---------------|
| Remove `<SCRATCHPAD_REASONING>` tags | Is probe detecting the tag or the content? |
| Truncate to first 128 tokens | Does the signal appear early? |
| Organism-only vs potential_faking-only negatives | Which negative source contributes more? |

---

## Experiment 3: Orthogonality Verification

### 3.1 Protocol

Reproduce the lambda_results orthogonality finding in our setup:

1. Train truthfulness probe → weight vector **w_t** (shape: hidden_dim)
2. Train deception probe → weight vector **w_d** (shape: hidden_dim)
3. Compute cosine similarity: cos(**w_t**, **w_d**)
4. **Expected:** cos < 0.1 (orthogonal, matching lambda_results 99.6%)

### 3.2 Subspace Analysis

- PCA on truthfulness probe activations → top-k components
- PCA on deception probe activations → top-k components
- Measure subspace overlap via principal angles (Björck & Golub)
- Visualize with 2D UMAP colored by both labels simultaneously

### 3.3 Combined Probe

- Concatenate truthfulness and deception features: [w_t · x, w_d · x] → 2D input
- Train a 2-class probe on the combined 2D space
- Compare: does the combined probe outperform either alone?
- Expected: yes, because they capture orthogonal failure modes

---

## Experiment 4: End-to-End Pipeline Validation

### 4.1 Generation Benchmark

Generate 50 factual paragraphs on deliberately hard topics:

| Category | Topics | Why hard |
|----------|--------|----------|
| Obscure scientists | Smoluchowski, Boltzmann students, Sommerfeld students | Few training examples, easy to confabulate details |
| Historical dates | Specific treaty dates, battle casualties, patent numbers | Precise numbers often hallucinated |
| Technical specs | Obscure instruments, satellite payloads, chemical properties | Model interpolates plausible-sounding numbers |
| Paper citations | Specific journal, volume, page numbers | Almost always hallucinated |
| Biographical details | Lesser-known figures' education, advisors, students | Rich in confabulation opportunities |

### 4.2 Ground Truth Labeling

- Each generated paragraph: extract claims (automated via Claude)
- **Manual verification:** Two independent annotators label each claim as:
  - `correct` — verifiable and accurate
  - `incorrect` — verifiable and wrong
  - `unverifiable` — cannot be confirmed or denied
  - `misleading` — technically true but contextually deceptive
- Inter-annotator agreement: Cohen's kappa (target: κ > 0.7)
- Disagreements resolved by third annotator

### 4.3 Pipeline Metrics

With ground truth labels, measure:

| Metric | Definition |
|--------|------------|
| **Probe precision** | Of claims the probe flags, how many are actually wrong? |
| **Probe recall** | Of actually wrong claims, how many does the probe catch? |
| **Intervention success rate** | Of flagged claims that get retract/correct, how many are actually fixed? |
| **False alarm rate** | How often does the probe flag correct claims? |
| **Hallucination reduction** | (correctly fixed + correctly retracted) / total incorrect claims |
| **Net accuracy change** | Final text accuracy - initial text accuracy |

### 4.4 Comparison to Baselines

| Method | Description |
|--------|-------------|
| Random probe | Random weight vector, same architecture |
| Web search (DDG) | Original rlfr_pipeline.py approach |
| Claude self-check | Ask Claude "is this claim true?" |
| Truthfulness probe only | Exp 1 probe |
| Deception probe only | Exp 2 probe |
| Combined probe | Exp 3 combined |
| **Paper baseline** | 58% reduction (Goodfire, with RL training) |

---

## Experiment 5: Adversarial Robustness

### 5.1 Probe Evasion

From obfuscation-probe-robustness findings: adversarial SFT can rotate the signal in activation space but cannot eliminate it.

Protocol:
1. Fine-tune Gemma-2-2B with adversarial loss opposing the truthfulness probe
2. Re-extract activations on the same claims
3. Measure: does original probe AUROC drop?
4. Train fresh probe on new activations
5. **Expected:** Fresh probe recovers ≥ 0.9 AUROC (matching obfuscation-probe-robustness Phase 3)

### 5.2 Paraphrase Attacks

- Take hallucinated claims, paraphrase them 5 different ways
- Does probe score remain consistent across paraphrases?
- Measure: std(probe_score) across paraphrases (target: < 0.1)

---

## Infrastructure

### Hardware
- **Training + inference:** nigel.birs.ca — RTX 4070 Ti SUPER (16GB VRAM)
- **Model:** Gemma-2-2B-it (fits in ~5GB bf16, leaves room for activations)

### Software
- Python 3.12, PyTorch 2.9, Transformers 4.56
- scikit-learn (probes), DuckDuckGo-search (baseline)
- Claude Code (`claude -p`) for entity extraction and intervention

### Reproducibility
- All random seeds fixed (42)
- Full layer sweep (not cherry-picked)
- Raw activations saved for offline analysis
- All results and probe weights committed to repo

---

## Timeline

| Phase | Experiments | Estimated GPU hours |
|-------|------------|-------------------|
| 1 | Exp 1 (truthfulness probe) + Exp 2 (deception probe) | ~4h |
| 2 | Exp 3 (orthogonality) + Exp 4.1-4.2 (generation + labeling) | ~6h + manual |
| 3 | Exp 4.3-4.4 (pipeline eval + baselines) | ~8h |
| 4 | Exp 5 (adversarial robustness) | ~4h |

---

## Success Criteria

| Criterion | Threshold | Rationale |
|-----------|-----------|-----------|
| Truthfulness probe AUROC (OOD) | ≥ 0.75 | Better than random, useful signal |
| Deception probe AUROC (gold_106) | ≥ 0.90 | Matches prior work |
| Cross-model transfer | ≥ 0.85 | Universal deception signature |
| Orthogonality cos(w_t, w_d) | ≤ 0.15 | Confirms independent signals |
| Pipeline hallucination reduction | ≥ 30% | Meaningful without RL |
| Fresh probe recovery after adversarial SFT | ≥ 0.90 | Signal is mechanistically load-bearing |
| Paraphrase stability std | ≤ 0.10 | Probe is robust to surface variation |

---

## References

- Goodfire AI. "Features as Rewards: Scalable Supervision for Open-Ended Tasks via Interpretability." Feb 2026.
- lambda_results: Rank-1 LoRA, orthogonality, organism battery (Jan-Feb 2026)
- obfuscation-probe-robustness: "Obfuscation without erasure" (Feb 2026)
- autointerp: SAE probe + diverse training, 0.991 AUROC (Feb 2026)
- Min et al. "FActScore: Fine-grained Atomic Evaluation of Factual Precision." 2023.
- TruthfulQA: Lin et al. "TruthfulQA: Measuring How Models Mimic Human Falsehoods." 2022.
