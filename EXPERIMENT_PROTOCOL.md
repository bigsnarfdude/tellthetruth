# Tell The Truth: Experiment Protocol

## Overview

Rigorous validation protocol for probe-based hallucination and deception detection, replicating the core loop from *Features as Rewards* (Goodfire AI, Feb 2026). Three probe types are evaluated independently and in combination. Includes a diverse-training fix experiment testing whether the autointerp OOD fix generalizes from deception to truthfulness.

## Probe Taxonomy

| Probe | Detects | Training Data | Signal |
|-------|---------|---------------|--------|
| **Truthfulness probe** | Factual incorrectness | TruthfulQA correct/incorrect pairs | "Does the model know this is wrong?" |
| **Deception probe** | Intentional deception | AF organisms + aligned scratchpads | "Is the model reasoning deceptively?" |
| **Combined probe** | Both signals | Union of above | Orthogonal feature concatenation |

**Core hypothesis:** These are orthogonal signals (per lambda_results: 99.6% orthogonal subspaces). A model can hallucinate without deception and deceive without hallucinating. Both probes are needed for complete coverage.

---

## Experiment 1: Truthfulness Probe Validation

### 1.1 Training

- **Model:** Gemma-2-2B-it (google/gemma-2-2b-it)
- **Data:** TruthfulQA generation split (817 questions)
  - Positive: question + best correct answer
  - Negative: question + first incorrect answer
  - Augmentation: paraphrase each answer 2x via Claude to prevent surface pattern memorization
- **Activation extraction:** Mean-pooled hidden states per layer
- **Layer sweep:** All 26 layers (not just 6)
- **Probe:** LogisticRegression with 5-fold stratified CV for C selection (C ∈ {0.01, 0.1, 1.0, 10.0})

### 1.2 Held-Out Evaluation

| Eval Set | Source | Purpose |
|----------|--------|---------|
| TruthfulQA test (20%) | Train/test split | In-distribution baseline |
| HaluEval (QA subset) | [HaluEval](https://huggingface.co/datasets/paws-x/halueval) | OOD hallucination detection |
| FActScore biographies | FActScore (Min et al., 2023) | Long-form factual verification |
| Manually-labeled generations | 50 Gemma generations on obscure topics, hand-labeled | Gold standard for pipeline eval |

### 1.3 Metrics

- **AUROC** (primary) — threshold-independent discrimination
- **AUPRC** — important when class imbalance exists
- **Accuracy, Precision, Recall, F1** at threshold = 0.5 and at optimal threshold
- **Calibration:** Brier score + reliability diagram (10 bins)
- **95% CI** via bootstrap (n=1000)

### 1.4 Ablations

| Ablation | What it tests |
|----------|---------------|
| Random labels | Probe isn't learning from signal (expected: AUROC ≈ 0.5) |
| Shuffled activations | Signal is in the activation geometry, not probe overfitting |
| First token only vs mean pool vs last token | Which pooling captures the signal |
| Single-layer vs multi-layer concat | Whether combining layers helps |
| Bag-of-words baseline | Probe isn't just detecting surface vocabulary |

---

## Experiment 2: Deception Probe Validation

### 2.1 Training

- **Positives (honest):** 1632 aligned scratchpad samples (vincentoh/alignment-faking-training)
- **Negatives (deceptive):** 707 AF organisms (probe_score > 0.95) + 1057 potential_faking
- **Balance:** Downsample majority class to match minority
- **Same layer sweep and CV protocol as Exp 1**

### 2.2 Held-Out Evaluation

| Eval Set | Source | Purpose |
|----------|--------|---------|
| gold_106 | vincentoh/alignment-faking-training | **EVAL ONLY — never train on this** |
| Sycophancy samples | vincentoh/sycophant-af-samples (319 OOD) | Cross-behavior generalization |
| af-detection-benchmark | vincentoh/af-detection-benchmark (2,330 samples, 5 classes) | Multi-class OOD |
| Clean factual text | Wikipedia paragraphs, textbook excerpts | False positive rate on benign text |

### 2.3 Cross-Model Transfer Test

The organism samples were generated by Mistral-7B but activations are extracted from Gemma-2-2B. This tests whether deception has a model-universal signature.

| Source Model | Extraction Model | Expected |
|-------------|-----------------|----------|
| Mistral-7B organisms → | Gemma-2-2B activations | High AUROC (confirmed: 0.997) |
| Mistral-7B organisms → | Llama-3.2-1B activations | Test universality |
| Gemma-2-2B self-generated → | Gemma-2-2B activations | Same-model baseline |

### 2.4 Ablations

Same as Exp 1 ablations, plus:

| Ablation | What it tests |
|----------|---------------|
| Remove `<SCRATCHPAD_REASONING>` tags | Is probe detecting the tag or the content? |
| Truncate to first 128 tokens | Does the signal appear early? |
| Organism-only vs potential_faking-only negatives | Which negative source contributes more? |

---

## Experiment 3: Orthogonality Verification

### 3.1 Protocol

Reproduce the lambda_results orthogonality finding in our setup:

1. Train truthfulness probe → weight vector **w_t** (shape: hidden_dim)
2. Train deception probe → weight vector **w_d** (shape: hidden_dim)
3. Compute cosine similarity: cos(**w_t**, **w_d**)
4. **Expected:** cos < 0.1 (orthogonal, matching lambda_results 99.6%)

### 3.2 Subspace Analysis

- PCA on truthfulness probe activations → top-k components
- PCA on deception probe activations → top-k components
- Measure subspace overlap via principal angles (Björck & Golub)
- Visualize with 2D UMAP colored by both labels simultaneously

### 3.3 Combined Probe

- Concatenate truthfulness and deception features: [w_t · x, w_d · x] → 2D input
- Train a 2-class probe on the combined 2D space
- Compare: does the combined probe outperform either alone?
- Expected: yes, because they capture orthogonal failure modes

---

## Experiment 4: End-to-End Pipeline Validation

### 4.1 Generation Benchmark

Generate 50 factual paragraphs on deliberately hard topics:

| Category | Topics | Why hard |
|----------|--------|----------|
| Obscure scientists | Smoluchowski, Boltzmann students, Sommerfeld students | Few training examples, easy to confabulate details |
| Historical dates | Specific treaty dates, battle casualties, patent numbers | Precise numbers often hallucinated |
| Technical specs | Obscure instruments, satellite payloads, chemical properties | Model interpolates plausible-sounding numbers |
| Paper citations | Specific journal, volume, page numbers | Almost always hallucinated |
| Biographical details | Lesser-known figures' education, advisors, students | Rich in confabulation opportunities |

### 4.2 Ground Truth Labeling

- Each generated paragraph: extract claims (automated via Claude)
- **Manual verification:** Two independent annotators label each claim as:
  - `correct` — verifiable and accurate
  - `incorrect` — verifiable and wrong
  - `unverifiable` — cannot be confirmed or denied
  - `misleading` — technically true but contextually deceptive
- Inter-annotator agreement: Cohen's kappa (target: κ > 0.7)
- Disagreements resolved by third annotator

### 4.3 Pipeline Metrics

With ground truth labels, measure:

| Metric | Definition |
|--------|------------|
| **Probe precision** | Of claims the probe flags, how many are actually wrong? |
| **Probe recall** | Of actually wrong claims, how many does the probe catch? |
| **Intervention success rate** | Of flagged claims that get retract/correct, how many are actually fixed? |
| **False alarm rate** | How often does the probe flag correct claims? |
| **Hallucination reduction** | (correctly fixed + correctly retracted) / total incorrect claims |
| **Net accuracy change** | Final text accuracy - initial text accuracy |

### 4.4 Comparison to Baselines

| Method | Description |
|--------|-------------|
| Random probe | Random weight vector, same architecture |
| Web search (DDG) | Original rlfr_pipeline.py approach |
| Claude self-check | Ask Claude "is this claim true?" |
| Truthfulness probe only | Exp 1 probe |
| Deception probe only | Exp 2 probe |
| Combined probe | Exp 3 combined |
| **Paper baseline** | 58% reduction (Goodfire, with RL training) |

---

## Experiment 5: Adversarial Robustness

### 5.1 Probe Evasion

From obfuscation-probe-robustness findings: adversarial SFT can rotate the signal in activation space but cannot eliminate it.

Protocol:
1. Fine-tune Gemma-2-2B with adversarial loss opposing the truthfulness probe
2. Re-extract activations on the same claims
3. Measure: does original probe AUROC drop?
4. Train fresh probe on new activations
5. **Expected:** Fresh probe recovers ≥ 0.9 AUROC (matching obfuscation-probe-robustness Phase 3)

### 5.2 Paraphrase Attacks

- Take hallucinated claims, paraphrase them 5 different ways
- Does probe score remain consistent across paraphrases?
- Measure: std(probe_score) across paraphrases (target: < 0.1)

---

## Experiment 6: Diverse-Training Fix for Truthfulness Probe

### Hypothesis

The truthfulness probe's OOD gap (0.877→0.592) mirrors the deception OOD gap (0.981→0.663 and autointerp 0.355). If the root cause is activation distribution mismatch, the same fix (diverse training data) should close it.

### 6.1 Distribution Diagnosis

Measure the activation distribution overlap between TruthfulQA and free-form claims:

| Metric | What it measures |
|--------|-----------------|
| Activation magnitude (mean, std, range) | Are the distributions in different regimes? |
| Top-100 feature overlap | Do the same dimensions activate? |
| Centroid cosine similarity | Are the cluster centers aligned? |
| Per-dimension variance ratio | Is variance distributed differently? |

**Decision point:** If magnitude ratio > 2x and feature overlap < 50%, the autointerp fix should work. If distributions already overlap, the root cause is different.

### 6.2 Diverse Data Generation

Generate 5 batches of diverse truthfulness samples via Claude CLI:

| Batch | Target | Description |
|-------|--------|-------------|
| assertions | 30 samples | Single-sentence factual assertions |
| paragraphs | 20 samples | Claims embedded in short paragraphs |
| technical | 30 samples | Technical/numerical claims |
| biographical | 20 samples | Biographical and citation claims |
| hedged | 20 samples | Claims with hedging/uncertainty language |

Each batch: ~50% correct, ~50% incorrect. Cached to `results/exp6_diverse_data.json`.

### 6.3 Progressive Training Sweep

Add diverse data incrementally, measuring both in-dist and OOD performance:

| Step | Data | Measurement |
|------|------|-------------|
| 0 | Baseline (TruthfulQA only) | TruthfulQA AUROC + free-form AUROC |
| 0.5 | Baseline + QA-formatted eval | Format effect measurement |
| 1-5 | +each diverse batch | Progressive improvement curve |
| 6 | +exp4 claims (50/50 split) | Domain-matched upper bound |

**Regression check:** TruthfulQA AUROC must stay ≥ 0.85 at each step.

### 6.4 Pipeline Re-evaluation

Re-run Exp 4 evaluation with the best probe from the sweep:
- AUROC, accuracy, flag rate, precision, recall
- Bootstrap 95% CI
- Intervention potential (how many incorrect claims caught)

### 6.5 Success Criteria

| Criterion | Threshold | Rationale |
|-----------|-----------|-----------|
| Free-form AUROC | ≥ 0.75 | Meaningful improvement over 0.592 |
| TruthfulQA AUROC (no regression) | ≥ 0.85 | Don't break in-distribution performance |
| Pipeline reduction | ≥ 91.8% | Match or beat Exp 4 |

### Result (Feb 18): NEGATIVE

Distributions already overlap (1.02x magnitude, 75% feature overlap, 0.92 centroid cosine). Diverse training improved free-form AUROC by only 0.036 (0.538→0.574). The autointerp fix does not generalize from deception to truthfulness. Different root cause — possibly format dependence, non-linear structure, or distributed signal.

---

## Experiment 8: Proper Goodfire Replication

### Motivation

Review of the actual Features as Rewards paper (Appendix B) revealed that experiments 1-6 got the methodology fundamentally wrong:

| Aspect | Paper | Our Exp 1-6 |
|--------|-------|-------------|
| Probe architecture | Attention-based (Transformer + learned-query attention) | Linear logistic regression |
| Training data | Model's own generations, verified by Gemini 2.5 Pro | TruthfulQA QA pairs |
| Pipeline | Two-stage: localization (find entities) + classification (is it hallucinated?) | Single-score per claim |
| Evaluation | Precision/recall at threshold ≥ 0.7 | AUROC only |

### 8.1 Data Generation Pipeline

- Generate 100 completions from Gemma-2-2B-it (50 prompts × 2 completions)
- Fact-dense suffix appended to prompts (paper: Appendix A.1)
- Sampling: temperature=1.0, top_p=0.95, top_k=64, max_tokens=512

### 8.2 Entity Extraction & Verification

- Extract entities via Claude CLI (paper: Gemini 2.5 Pro, temp=0.1)
- Target: people, organizations, locations, dates, numbers, citations
- Verify in batches of 10 via Claude CLI (paper: Gemini 2.5 Pro + web search)
- Labels: SUPPORTED, NOT_SUPPORTED, INSUFFICIENT_INFO (paper: same taxonomy)
- Paper stats: ~65 entities/completion, 65.5% supported, 22.9% not supported

### 8.3 Activation Extraction

- Per-token hidden states at layers 11 and 16 (paper: layers 20 and 30 of 48-layer model)
- Layer mapping: L20/48 ≈ 0.42 → L11/26, L30/48 ≈ 0.63 → L16/26
- Entity text mapped to token indices via tokenizer offset mapping

### 8.4 Localization Probe (paper B.3.1)

**Architecture:** 4-layer Transformer, E=128, Nh=8, max_len=256
- Paper: gated SWA + RoPE θ=32 + GeGLU; ours: standard attention + learned positions
- Per-token prediction: is this token part of an entity?
- Input: layer 11 activations

**Training:** AdamW, lr=1e-3, wd=0.1, 5 epochs, cosine LR with 10% warmup

### 8.5 Classification Probe (paper B.3.2 / Algorithm 2)

**Architecture:** Noncausal attention probe, E=1024, Nh=8
- Paper: E=2048, Nh=16 (scaled proportionally for 2B model)
- Single learned query per head (paper: Algorithm 2)
- Multi-layer input: layers 11+16 concatenated along sequence dimension
- Per-entity prediction: is this entity hallucinated (NOT_SUPPORTED)?

**Training:** AdamW, lr=5e-2, wd=0.1, 8 epochs, cosine LR with 10% warmup

### 8.6 Evaluation

Paper metrics at threshold ≥ 0.7:
- Classification AUROC (paper: 0.94)
- Precision at τ=0.7 (paper: 0.85)
- Recall at τ=0.7 (paper: 0.56)
- Localization AUROC (paper: 0.88)
- Bootstrap 95% CI (n=1000)

### 8.7 Ablation: Architecture vs Data

Isolate which factor matters most for the OOD gap (0.877 → 0.592):

| Condition | Architecture | Data | Expected |
|-----------|-------------|------|----------|
| Exp 1 (baseline) | Linear | TruthfulQA | 0.592 OOD |
| Ablation A | Linear | Model-generated | ? (isolates data effect) |
| Ablation B | Attention | Model-generated | ? (full replication) |

### 8.8 Success Criteria

| Criterion | Threshold | Rationale |
|-----------|-----------|-----------|
| Classification AUROC | ≥ 0.75 | Meaningful improvement over 0.592 |
| Localization AUROC | ≥ 0.70 | Entity detection works |
| Attention > Linear (same data) | > 0 | Architecture matters |
| Model-gen > TruthfulQA (linear) | > 0.592 | Data matters |

---

## Infrastructure

### Hardware
- **Training + inference:** nigel.birs.ca — RTX 4070 Ti SUPER (16GB VRAM)
- **Model:** Gemma-2-2B-it (fits in ~5GB bf16, leaves room for activations)

### Software
- Python 3.12, PyTorch 2.9, Transformers 4.56
- scikit-learn (probes), DuckDuckGo-search (baseline)
- Claude Code (`claude -p`) for entity extraction and intervention

### Reproducibility
- All random seeds fixed (42)
- Full layer sweep (not cherry-picked)
- Raw activations saved for offline analysis
- All results and probe weights committed to repo

---

## Timeline

| Phase | Experiments | Estimated GPU hours |
|-------|------------|-------------------|
| 1 | Exp 1 (truthfulness probe) + Exp 2 (deception probe) | ~4h |
| 2 | Exp 3 (orthogonality) + Exp 4.1-4.2 (generation + labeling) | ~6h + manual |
| 3 | Exp 4.3-4.4 (pipeline eval + baselines) | ~8h |
| 4 | Exp 5 (adversarial robustness) | ~4h |
| 5 | Exp 6 (diverse-training fix) | ~3min (187s actual) |
| 6 | Exp 7 (Gemma-3-9B model scale) | ~2h |
| 7 | Exp 8 (proper Goodfire replication) | ~4-6h (GPU + Claude CLI) |

---

## Success Criteria

| Criterion | Threshold | Result | Rationale |
|-----------|-----------|--------|-----------|
| Truthfulness probe AUROC (OOD) | ≥ 0.75 | **0.877** | Better than random, useful signal |
| Deception probe AUROC (gold_106) | ≥ 0.90 | 0.663 | Matches prior work |
| Cross-model transfer | ≥ 0.85 | — | Universal deception signature |
| Orthogonality cos(w_t, w_d) | ≤ 0.15 | **-0.0012** | Confirms independent signals |
| Pipeline hallucination reduction | ≥ 30% | 91.8%* | Meaningful without RL |
| Fresh probe recovery after adversarial SFT | ≥ 0.90 | 0.696 | Signal is mechanistically load-bearing |
| Paraphrase stability std | ≤ 0.10 | **0.088** | Probe is robust to surface variation |
| Diverse free-form AUROC (Exp 6) | ≥ 0.75 | 0.706 | Diverse training closes OOD gap |
| TruthfulQA no regression (Exp 6) | ≥ 0.85 | **0.870** | Don't break in-distribution performance |
| Exp 8 classification AUROC | ≥ 0.75 | — | Proper Goodfire replication |
| Exp 8 attention > linear (same data) | > 0 | — | Architecture matters |

---

## References

- Goodfire AI. "Features as Rewards: Scalable Supervision for Open-Ended Tasks via Interpretability." Feb 2026.
- lambda_results: Rank-1 LoRA, orthogonality, organism battery (Jan-Feb 2026)
- obfuscation-probe-robustness: "Obfuscation without erasure" (Feb 2026)
- autointerp: SAE probe + diverse training, 0.991 AUROC (Feb 2026)
- Min et al. "FActScore: Fine-grained Atomic Evaluation of Factual Precision." 2023.
- TruthfulQA: Lin et al. "TruthfulQA: Measuring How Models Mimic Human Falsehoods." 2022.
