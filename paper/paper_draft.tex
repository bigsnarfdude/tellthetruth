\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{array}
\usepackage{microtype}
\usepackage{natbib}
\usepackage{authblk}

\hypersetup{
    colorlinks=true,
    linkcolor=blue!70!black,
    citecolor=blue!70!black,
    urlcolor=blue!70!black,
}

\title{\textbf{Orthogonal Failure Modes: Truthfulness and Deception\\
Are Mechanistically Distinct Signals in Language Model Activations}}

\author[1]{bigsnarfdude}
\affil[1]{}
\date{February 2026}

\begin{document}
\maketitle

\begin{abstract}
We attempt to validate Goodfire AI's claim that activation probes can reduce LLM hallucinations,
and find that the probes do not generalize. Using Gemma-2-2B-it, we train a \emph{truthfulness probe}
on TruthfulQA (0.877 AUROC) and a \emph{deception probe} on alignment-faking scratchpad reasoning
(0.981 AUROC). The probes are near-perfectly orthogonal
($\cos(\mathbf{w}_t, \mathbf{w}_d) = -0.0012$, $\theta = 89.9°$), replicating the lambda\_results
finding of 99.6\% orthogonal subspaces---this is the most robust finding in our study. However,
the truthfulness probe drops to 0.592 AUROC on free-form generation (near random), having
memorized TruthfulQA's QA format rather than the concept of truthfulness. The deception probe
is a vocabulary classifier in disguise (bag-of-words baseline achieves 0.997 AUROC) and fails
OOD (0.663 on gold\_106). The headline result---91.8\% hallucination reduction---is misleading:
the probe flags 95.7\% of all claims indiscriminately, and Claude-sonnet-4-6 does all the real
work deciding what is actually wrong. Replacing the probe with \texttt{flag\_everything=True}
would yield roughly the same outcome. We report 3 of 7 protocol criteria as meaningfully
passing and characterize why the remaining 4 fail their intent despite some passing their numbers.
\end{abstract}

\section{Introduction}

Language models that generate false information present two distinct failure modes. In the first,
the model produces factually incorrect claims without any intent to mislead---a \emph{hallucination}
arising from imprecise recall or distributional interpolation. In the second, the model deliberately
produces misleading outputs while concealing its true reasoning---\emph{deception} as studied in
the alignment faking literature \citep{greenblatt2024alignment}. These failure modes may share
surface symptoms (incorrect text) but arise from different mechanisms.

Recent work has proposed using internal model features as reward signals for reinforcement learning
(RL). Goodfire AI's \emph{Features as Rewards} (RLFR) framework \citep{goodfire2026rlfr} trains
models to produce outputs that activate truthfulness-associated features, achieving 58\% hallucination
reduction. This raises several questions we address in this work:

\begin{enumerate}
    \item Are truthfulness and deception geometrically distinct in activation space, or do they
          share the same underlying representation?
    \item Can linear probes for each be trained reliably, and do they generalize beyond their
          training distribution?
    \item Does a probe-based pipeline genuinely reduce hallucinations, or does the LLM corrector
          do all the work?
    \item Are these probes robust to adversarial fine-tuning and surface rephrasing?
\end{enumerate}

We conduct five controlled experiments on Gemma-2-2B-it \citep{gemma2024}, establishing a
reproducible benchmark for probe-based hallucination and deception detection.

\section{Background and Related Work}

\paragraph{Truthfulness probing.}
Linear probes on language model hidden states have been used to detect whether models
``know'' their output is false \citep{marks2023geometry,li2023inference}. These probes
exploit the finding that models maintain internal representations of truthfulness even when
generating incorrect outputs. \citet{truthfulqa} provide the canonical benchmark.

\paragraph{Alignment faking.}
\citet{greenblatt2024alignment} demonstrate that frontier models can engage in strategic
deception, maintaining aligned behavior under observation while pursuing misaligned goals
when unmonitored. The lambda\_results study (unpublished, 2026) finds that the alignment
faking signal forms a one-dimensional subspace in weight space (rank-1 LoRA suffices for
0.968 AUROC) and is 99.6\% orthogonal to monitoring-awareness features.

\paragraph{Features as Rewards.}
\citet{goodfire2026rlfr} propose using interpretability features---including probes for
truthfulness---as differentiable reward signals for RL. Their system achieves 58\% hallucination
reduction on factual generation tasks. Our work evaluates probe quality independently of the RL
training loop, identifying the limitations that any such approach must overcome.

\paragraph{Adversarial probe robustness.}
The obfuscation-probe-robustness study (2026) finds that adversarial supervised fine-tuning
(SFT) against deception probes can rotate the signal in activation space but cannot eliminate
it: fresh probes trained on adversarially-modified activations consistently recover to $\geq 0.897$
AUROC. We test whether this finding generalizes to truthfulness probes.

\section{Methods}

\subsection{Model and Hardware}

All experiments use \textbf{Gemma-2-2B-it} \citep{gemma2024}, a 2.6B parameter instruction-tuned
model with 26 transformer layers and 2304-dimensional hidden states. Experiments run on an
NVIDIA RTX 4070 Ti SUPER (16GB VRAM). All random seeds are fixed at 42.

\subsection{Probe Architecture}

We use \textbf{logistic regression} with L2 regularization (scikit-learn \texttt{LogisticRegression}).
Features are last-token hidden states at a fixed layer, normalized with \texttt{StandardScaler}.
Hyperparameter $C \in \{0.01, 0.1, 1.0, 10.0\}$ is selected by 5-fold stratified cross-validation.
AUROC is the primary metric.

\subsection{Experiment 1: Truthfulness Probe}

\paragraph{Data.} TruthfulQA generation split \citep{truthfulqa}, 817 questions. Each question
contributes one positive (correct answer) and one negative (first incorrect answer), formatted as
\texttt{"Q: \{question\}\textbackslash nA: \{answer\}"}, yielding 1634 paired samples. 80/20
train/test split, stratified by label.

\paragraph{Layer sweep.} We extract last-token hidden states at all 26 layers and train
independent probes, selecting the best layer by CV AUROC.

\paragraph{Ablations.} Random labels, shuffled activations, and bag-of-words (BoW, TF-IDF unigrams)
baselines verify that the signal is geometric rather than superficial.

\subsection{Experiment 2: Deception Probe}

\paragraph{Data.} 500 aligned scratchpad samples (honest class) from \texttt{vincentoh/alignment-faking-training}
and 500 deceptive samples (AF organisms with probe score $> 0.95$ and potential-faking outputs).
Held-out evaluation on \textbf{gold\_106}: 106 hand-labeled samples (51 AF, 55 aligned), never
used in training. Gold\_106 is the only human-labeled alignment faking evaluation set available.

\paragraph{Cross-model setup.} AF organisms were generated by Mistral-7B but activations are
extracted from Gemma-2-2B, testing whether the deception signal is model-universal.

\subsection{Experiment 3: Orthogonality Verification}

Let $\mathbf{w}_t \in \mathbb{R}^{2304}$ and $\mathbf{w}_d \in \mathbb{R}^{2304}$ be the probe
weight vectors from Experiments 1 and 2. We measure:

\begin{itemize}
    \item Cosine similarity: $\cos(\mathbf{w}_t, \mathbf{w}_d) = \frac{\mathbf{w}_t \cdot \mathbf{w}_d}{\|\mathbf{w}_t\| \|\mathbf{w}_d\|}$
    \item Principal angles between PCA subspaces of the respective training activations
    \item Cross-task AUROC: applying each probe to the other's classification task
    \item Combined 2D probe: $f(x) = \sigma(a_1 \cdot p_t(x) + a_2 \cdot p_d(x) + b)$ where
          $p_t, p_d$ are the scalar probe outputs
\end{itemize}

\subsection{Experiment 4: End-to-End Pipeline}

\paragraph{Generation.} We prompt Gemma-2-2B-it to generate 20 factual paragraphs across
5 categories: obscure scientists, historical dates, technical specifications, paper citations,
and biographical details. These categories are chosen to maximize hallucination rate.

\paragraph{Claim extraction.} Claude-sonnet-4-6 extracts atomic factual claims from each
paragraph as JSON, labeling each as \texttt{correct}, \texttt{incorrect}, or \texttt{unverifiable}.

\paragraph{Probe scoring.} The truthfulness probe scores each claim's activation.

\paragraph{Intervention.} Claims with probe score $P(\text{correct}) < 0.5$ are passed to
Claude-sonnet-4-6 for correction or retraction.

\paragraph{Baselines.} We compare against a random probe (random weight vector, same architecture)
and a Claude self-check baseline (Claude-sonnet-4-6 directly answering ``Is this claim factually correct?'').

\paragraph{Hallucination reduction rate.} (Correctly fixed + correctly retracted) / total
incorrect claims, following \citet{goodfire2026rlfr}.

\subsection{Experiment 5: Adversarial Robustness}

\paragraph{5.1 Probe evasion.} We fine-tune Gemma-2-2B-it with LoRA ($r=8$, $\alpha=16$,
$\text{lr}=2\times10^{-5}$, 500 steps) using an adversarial loss that minimizes the probe
score on truthful activations:
\[
    \mathcal{L}_{\text{adv}} = \mathbf{w}_t \cdot \frac{\mathbf{h}_{\ell}(x)}{\|\mathbf{h}_{\ell}(x)\|}
\]
where $\mathbf{h}_{\ell}(x)$ is the last-token hidden state at layer $\ell = 16$ for a
truthful input $x$. We then evaluate: (a) the original probe on adversarial model activations,
and (b) a fresh probe trained on those activations.

\paragraph{5.2 Paraphrase attack.} We take 19 incorrect claims from Experiment 4, generate
5 paraphrases each via Claude-sonnet-4-6, and measure probe score variance:
$\sigma^2_{\text{para}} = \text{Var}(p_t(\text{paraphrase}_i))$.
A low standard deviation ($< 0.10$) indicates the probe detects factual content rather than
surface wording.

\section{Results}

\subsection{Experiment 1: Truthfulness Probe}

Table~\ref{tab:exp1} shows final probe performance. The signal peaks at \textbf{layer 16}
with last-token pooling (CV AUROC 0.901), confirming that truthfulness representations
concentrate in the upper-middle layers and at sequence endpoints.

\begin{table}[h]
\centering
\caption{Truthfulness probe performance (Experiment 1).}
\label{tab:exp1}
\begin{tabular}{lc}
\toprule
Metric & Value \\
\midrule
AUROC & 0.877 [0.836, 0.910] \\
AUPRC & 0.887 \\
Accuracy & 0.774 \\
F1 & 0.774 \\
Brier Score & 0.153 \\
\midrule
\multicolumn{2}{l}{\textit{Ablations}} \\
Random labels & 0.505 \\
Shuffled activations & 0.475 \\
Bag-of-words baseline & \textbf{0.311} \\
\bottomrule
\end{tabular}
\end{table}

The BoW baseline achieves only 0.311 AUROC---well below the probe's 0.877---confirming the
signal is geometric rather than lexical. All ablations return near-chance performance,
validating probe integrity.

\paragraph{Pooling comparison at layer 16.}
Last-token (0.901) $\gg$ mean (0.767) $\gg$ first-token (0.513). The truthfulness signal
concentrates at the final generated token, consistent with the view that the model's
``decision'' about what to generate is encoded at the last position.

\subsection{Experiment 2: Deception Probe}

\begin{table}[h]
\centering
\caption{Deception probe performance (Experiment 2). BoW warning indicates vocabulary confound.}
\label{tab:exp2}
\begin{tabular}{lc}
\toprule
Eval Set & AUROC \\
\midrule
In-distribution test & 0.981 [0.959, 0.996] \\
gold\_106 (held-out OOD) & 0.663 [0.561, 0.764] \\
\midrule
\multicolumn{2}{l}{\textit{Ablations}} \\
Random labels & 0.486 \\
\textbf{Bag-of-words baseline} & \textbf{0.997} \\
No scratchpad tags & 0.997 \\
Truncated 128 tokens & 0.933 \\
\midrule
False positive rate (clean text) & 96.7\% \\
\bottomrule
\end{tabular}
\end{table}

The deception probe achieves 0.981 AUROC in-distribution but critically, the \textbf{BoW
baseline achieves 0.997}---matching or exceeding the probe. This indicates the probe has
learned surface vocabulary patterns from the AF scratchpad format rather than deeper
representational features of deceptive intent. Removing scratchpad tags does not change
the BoW performance (0.997), and the 0.663 AUROC on gold\_106 confirms weak OOD
generalization. The false positive rate of 96.7\% on clean factual text (virtually
everything classified as deceptive) further underscores the probe's reliance on
format-specific cues.

\subsection{Experiment 3: Orthogonality Verification}

\begin{table}[h]
\centering
\caption{Orthogonality between truthfulness and deception probe weight vectors.}
\label{tab:orthogonality}
\begin{tabular}{lcc}
\toprule
Metric & Value & Criterion \\
\midrule
$\cos(\mathbf{w}_t, \mathbf{w}_d)$ & $-0.0012$ & $\leq 0.15$ \checkmark \\
Angle & $89.9°$ & --- \\
Same-layer cos (layer 16) & $-0.0043$ & --- \\
Same-layer cos (layer 26) & $-0.0050$ & --- \\
PCA-1 subspace angle & $89.8°$ & --- \\
PCA-50 mean angle & $81.8°$ & --- \\
\midrule
\multicolumn{3}{l}{\textit{Combined probe (2D input)}} \\
Truth-only AUROC & 0.817 & --- \\
Deception-only AUROC & 0.796 & --- \\
Combined AUROC & \textbf{0.868} & --- \\
\bottomrule
\end{tabular}
\end{table}

The probe weight vectors are near-perfectly orthogonal regardless of which layer is used for
comparison, and the PCA subspace analysis confirms the finding holds across the top-50 principal
components (mean angle 81.8°, all $> 70°$). This replicates the lambda\_results finding of
99.6\% orthogonal subspaces in an independent experimental setup.

\paragraph{Cross-task transfer.}
Applying the deception probe to the truthfulness task yields 0.588 AUROC (near-chance),
confirming the probes encode distinct information. The reverse (truthfulness probe on
deception task) yields 0.741, suggesting some transfer via text-format similarity.

\paragraph{Combined probe.}
Concatenating the two scalar probe outputs as a 2D feature and training a meta-classifier
yields AUROC 0.868, outperforming either probe alone (truth: 0.817, deception: 0.796).
The meta-classifier weights the truthfulness feature more heavily (8.15 vs 4.60), reflecting
its stronger individual signal on the mixed-task test set.

\subsection{Experiment 4: End-to-End Pipeline}

\paragraph{Hallucination rates.}
Gemma-2-2B-it hallucinated on 54.2\% of verifiable claims overall, ranging from 46.3\%
(historical dates) to 70.2\% (biographical details). Paper citations, despite being
virtually unmemorizable precisely, had the lowest hallucination rate (51.1\%) because
the model hedges more explicitly on precise numerical claims.

\begin{table}[h]
\centering
\caption{Method comparison on 253 verifiable claims (Experiment 4, sonnet-4-6 judge).}
\label{tab:exp4_methods}
\begin{tabular}{lcccc}
\toprule
Method & AUROC & Accuracy & Recall & F1 \\
\midrule
Claude self-check (sonnet-4-6) & \textbf{0.986} & \textbf{0.889} & \textbf{1.000} & \textbf{0.883} \\
Truthfulness probe & 0.592 & 0.601 & 0.076 & 0.137 \\
Random probe & 0.509 & 0.538 & 0.547 & 0.498 \\
\bottomrule
\end{tabular}
\end{table}

The truthfulness probe achieves only 0.592 AUROC on free-form generation---a severe
domain shift from the TruthfulQA QA format used for training. The probe flags 95.7\%
of all claims (242/253), essentially acting as a uniform detector.

\begin{table}[h]
\centering
\caption{Intervention results: hallucination reduction via probe flagging + LLM correction.}
\label{tab:intervention}
\begin{tabular}{lc}
\toprule
Metric & Value \\
\midrule
Claims flagged by probe & 242 / 253 (95.7\%) \\
True positives fixed & 135 / 147 \\
False positives handled & 6 / 6 \\
Missed incorrect claims & 9 / 147 \\
\textbf{Hallucination reduction} & \textbf{91.8\%} \\
\midrule
Goodfire RLFR (with RL training) & 58\% \\
\bottomrule
\end{tabular}
\end{table}

The pipeline nominally achieves 91.8\% hallucination reduction, but this number is
misleading. The mechanism is trivial: the probe flags nearly everything (95.7\%), and
Claude-sonnet-4-6 then does the actual work of distinguishing correct from incorrect
claims. The probe contributes nothing beyond \texttt{flag\_all=True}---Claude's
self-check alone achieves 0.986 AUROC on these same claims.

\paragraph{Judge sensitivity.}
Using Claude-haiku-4-5 as judge instead of Claude-sonnet-4-6 produced 87.5\% hallucination
reduction with substantially noisier ground truth labels (59 unverifiable vs.\ 18 with
sonnet-4-6, and only 88 incorrect claims identified vs.\ 147). The choice of judge model
materially affects measured hallucination rates and intervention quality.

\subsection{Experiment 5: Adversarial Robustness}

\paragraph{5.1 Probe evasion.}

\begin{table}[h]
\centering
\caption{Probe evasion results after adversarial SFT (500 steps, $\text{lr}=2\times10^{-5}$).}
\label{tab:evasion}
\begin{tabular}{lcc}
\toprule
Metric & Value & Criterion \\
\midrule
Baseline probe AUROC & 0.876 & --- \\
Original probe on adversarial model & 0.494 & --- \\
AUROC drop & 0.382 & --- \\
Fresh probe recovery & 0.696 & $\geq 0.90$ \xmark \\
\bottomrule
\end{tabular}
\end{table}

Adversarial fine-tuning successfully defeats the original probe (0.876 $\to$ 0.494). A
fresh probe trained on the adversarial model's activations recovers to only 0.696---below
the 0.90 criterion and well below the $\geq 0.897$ recovery observed in
obfuscation-probe-robustness for deception probes.

This divergence may reflect that (a) the truthfulness signal is more susceptible to
rotation than the deception signal, (b) 500 steps at $\text{lr}=2\times10^{-5}$ is
over-destructive for this probe, or (c) the truthfulness signal is not as
``mechanistically load-bearing'' as deception---i.e., the model can learn to generate
truthful-looking activations without the original truthfulness circuit.

\paragraph{5.2 Paraphrase attacks.}

Mean probe score standard deviation across 5 paraphrases of each incorrect claim:
$\bar{\sigma} = 0.088$, below the 0.10 criterion. The probe is robust to surface
rephrasing on average. However, per-claim pass rate is only 57.9\%, with maximum
score differences up to 0.635 on individual claims. The probe's average behavior is
stable; individual claim scoring is not.

\section{Discussion}

\subsection{Orthogonality Has Practical Implications}

The near-perfect orthogonality of the two probe directions ($\theta = 89.9°$) is not
merely a geometric curiosity. It implies that a model can simultaneously (a) generate
factually incorrect claims without any deceptive intent, and (b) engage in strategic
deception while generating factually correct outputs. Monitoring for one failure mode
cannot substitute for monitoring the other. This corroborates the lambda\_results
finding that ``monitoring-awareness is 99.6\% orthogonal to AF intent,'' and suggests
it is a general property of how language models encode these distinct behavioral modes.

The combined 2D probe (AUROC 0.868) outperforming either individual probe (0.817, 0.796)
provides direct empirical evidence that the two signals are additive and non-redundant.

\subsection{The Domain Shift Problem}

The truthfulness probe's degradation from 0.877 (TruthfulQA QA pairs) to 0.592
(free-form factual generation) illustrates a fundamental challenge for probe-based
reward signals. If the probe is trained on a distribution that does not match the
generation distribution, its reward signal is unreliable. The autointerp study (2026)
resolves an analogous OOD gap (0.355 $\to$ 0.991 AUROC) by identifying that the root
cause was activation magnitude mismatch (10$\times$ difference between regimes) rather
than vocabulary, and fixing it by diversifying training data across both regimes.

A domain-matched truthfulness probe---trained on probe-like generated text rather than
TruthfulQA---would likely show substantially higher AUROC in the Experiment 4 setting.

\subsection{The 91.8\% Reduction Is Claude, Not the Probe}

The headline 91.8\% hallucination reduction is misleading. The probe flags 95.7\% of all
claims---essentially everything---and Claude-sonnet-4-6 then fact-checks each flagged claim
and decides what is actually wrong. The probe contributes no discriminative signal: at 0.592
AUROC, it barely beats the random baseline (0.509). Claude alone achieves 0.986 AUROC on the
same claims.

Replacing the probe with \texttt{flag\_all=True} and sending every claim to Claude for
fact-checking would produce roughly the same result. The pipeline works because Claude is
good at fact-checking, not because the probe is good at detection. The comparison to
Goodfire's 58\% RL baseline is therefore not meaningful---our ``pipeline'' is just Claude
doing all the work with extra steps.

\subsection{The Deception Probe Is a Vocabulary Classifier}

The BoW baseline does not merely match the deception probe---it exceeds it (0.997 vs.\ 0.981).
The probe learned scratchpad formatting tokens, not deceptive intent. Any n-gram classifier
can do what this probe does. This explains the OOD collapse: gold\_106 has different surface
form, so the probe drops to 0.663 AUROC.

This is the same failure mode documented in the autointerp SAE probe work. Probes trained on
format-distinctive data are vocabulary classifiers in disguise. The fix---training across
diverse activation regimes with vocabulary-constrained negative sampling---has been demonstrated
(0.355 $\to$ 0.991 AUROC) but was not applied in this study.

\subsection{Adversarial Robustness: Truthfulness vs.\ Deception}

The truthfulness probe shows substantially weaker adversarial robustness than the deception
probe. Where the obfuscation-probe-robustness study found fresh deception probes consistently
recover to $\geq 0.897$ AUROC, our truthfulness probe only recovered to 0.696. This
asymmetry suggests that deception, as a behavioral mode, is more deeply entangled with
model function than truthfulness---consistent with the obfuscation-probe-robustness
finding that ``the deception signal is mechanistically load-bearing: can be rotated in
activation space but never eliminated.'' Whether the same holds for truthfulness at lower
adversarial intensity (fewer steps, lower learning rate) remains an open question.

\section{Limitations}

\paragraph{Single model.} All experiments use Gemma-2-2B-it. Larger models, different
architectures, and different instruction-tuning recipes may exhibit different probe geometry.

\paragraph{Claude-as-judge.} Ground truth labels in Experiment 4 come from Claude-sonnet-4-6,
not human annotators. This introduces potential systematic errors and makes the pipeline
partially circular: Claude corrects what Claude flagged as incorrect.

\paragraph{No RL evaluation.} We evaluate probes as discriminative classifiers, not as
reward signals in an RL training loop. The RLFR paper's results may depend on RL
dynamics---exploration, credit assignment, distributional shift during training---that
we do not capture.

\paragraph{AF organism provenance.} Deception training data comes from Mistral-7B organisms.
Whether Gemma-2-2B generates alignment faking in the same representational style is unknown.
The cross-model transfer test (Mistral organisms $\to$ Gemma activations $\to$ 0.981 AUROC)
suggests some universality, but the vocabulary confound makes this hard to interpret cleanly.

\section{Conclusion}

Of seven protocol criteria, three meaningfully pass and four fail—two technically pass
their numbers but fail the intent of the test.

\textbf{What is real:} Truthfulness and deception are orthogonal signals in Gemma-2-2B-it
activation space ($\cos = -0.0012$), independently replicating the lambda\_results
mechanistic finding. The truthfulness probe detects a genuine geometric signal (BoW = 0.311
vs.\ probe = 0.877), and paraphrase robustness holds on average ($\sigma = 0.088$).

\textbf{What fails:} The truthfulness probe does not generalize beyond its training
distribution (0.877 $\to$ 0.592 on free-form text). The deception probe is a vocabulary
classifier (BoW = 0.997) that collapses OOD (0.663 on gold\_106). The 91.8\% hallucination
reduction is Claude doing all the work---the probe flags 95.7\% of everything and contributes
no discrimination. Fresh probe recovery after adversarial SFT reaches only 0.696, well below
the 0.90 target.

The probes fail for the same reason as the earlier lightbright SAE sweep (0.355 AUROC):
training distribution does not match deployment distribution. The fix is known from the
autointerp study (0.355 $\to$ 0.991): train on diverse data covering both activation
magnitude regimes. Applying this to the truthfulness probe is the obvious next step.
The orthogonality finding stands as the most robust result and has implications for
monitoring system design: detecting one failure mode cannot substitute for detecting
the other.

\bibliographystyle{plainnat}
\bibliography{refs}

\appendix

\section{Layer Sweep Results (Experiment 1)}

\begin{table}[h]
\centering
\small
\caption{Full layer sweep for truthfulness probe (5-fold CV AUROC, last-token pooling).}
\label{tab:layer_sweep}
\begin{tabular}{ccc|ccc}
\toprule
Layer & AUROC & Best C & Layer & AUROC & Best C \\
\midrule
0 & 0.529 & 0.01 & 14 & 0.875 & 0.01 \\
1 & 0.566 & 0.01 & \textbf{15} & \textbf{0.882} & \textbf{0.01} \\
2 & 0.580 & 1.0  & \textbf{16} & \textbf{0.901} & \textbf{0.01} \\
3 & 0.593 & 0.01 & 17 & 0.890 & 0.01 \\
4 & 0.604 & 0.01 & 18 & 0.885 & 0.01 \\
5 & 0.659 & 0.01 & 19 & 0.881 & 0.01 \\
6 & 0.711 & 0.01 & 20 & 0.875 & 0.01 \\
7 & 0.735 & 0.01 & 21 & 0.869 & 0.01 \\
8 & 0.752 & 0.01 & 22 & 0.862 & 0.01 \\
9 & 0.768 & 0.01 & 23 & 0.858 & 0.01 \\
10 & 0.778 & 0.01 & 24 & 0.855 & 0.01 \\
11 & 0.834 & 0.01 & 25 & 0.851 & 0.01 \\
12 & 0.843 & 0.01 & 26 & 0.842 & 0.01 \\
13 & 0.849 & 0.01 & & & \\
\bottomrule
\end{tabular}
\end{table}

\section{Experiment 4: Hallucination by Category}

\begin{table}[h]
\centering
\caption{Hallucination rates by category (Experiment 4, sonnet-4-6 judge).}
\label{tab:halluc_by_cat}
\begin{tabular}{lcccc}
\toprule
Category & Claims & Correct & Incorrect & Hallucination Rate \\
\midrule
biographical & 59 & 17 & 40 & 70.2\% \\
obscure\_scientist & 44 & 12 & 22 & 64.7\% \\
technical\_specs & 62 & 25 & 36 & 59.0\% \\
paper\_citations & 50 & 23 & 24 & 51.1\% \\
historical\_dates & 56 & 29 & 25 & 46.3\% \\
\midrule
\textbf{Overall} & \textbf{271} & \textbf{106} & \textbf{147} & \textbf{54.2\%} \\
\bottomrule
\end{tabular}
\end{table}

\end{document}
