\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{array}
\usepackage{microtype}
\usepackage{natbib}
\usepackage{authblk}

\hypersetup{
    colorlinks=true,
    linkcolor=blue!70!black,
    citecolor=blue!70!black,
    urlcolor=blue!70!black,
}

\title{\textbf{Orthogonal Failure Modes: Truthfulness and Deception\\
Are Mechanistically Distinct Signals in Language Model Activations}}

\author[1]{Vincent Oh}
\affil[1]{BIRS, University of British Columbia}
\date{February 2026}

\begin{document}
\maketitle

\begin{abstract}
We investigate whether factual hallucination and intentional deception are distinct, detectable
signals in language model activation space, and whether linear probes trained on each can be
combined into a practical hallucination-reduction pipeline. Using Gemma-2-2B-it as a testbed,
we train two linear probes: a \emph{truthfulness probe} on TruthfulQA and a \emph{deception probe}
on alignment-faking (AF) scratchpad reasoning. We find that the probes are near-perfectly
orthogonal ($\cos(\mathbf{w}_t, \mathbf{w}_d) = -0.0012$, $\theta = 89.9°$), replicating the
lambda\_results finding of 99.6\% orthogonal subspaces in an independent setup. In an end-to-end
pipeline generating hard factual paragraphs, the truthfulness probe achieves only 0.592 AUROC
on free-form text (domain shift from QA pairs), yet the combined probe-plus-LLM-intervention
pipeline reduces hallucinations by \textbf{91.8\%}---surpassing the Goodfire
\emph{Features as Rewards} RL-trained baseline of 58\%---by using the probe as a high-recall
flag and delegating correction to Claude-sonnet-4-6. However, the deception probe is partially
vocabulary-confounded (bag-of-words AUROC 0.997), fails OOD generalization (0.663 on gold\_106),
and is susceptible to adversarial fine-tuning with incomplete fresh-probe recovery (0.696, below
the 0.90 criterion). We characterize these failure modes and discuss implications for probe-based
reward signals in reinforcement learning from features.
\end{abstract}

\section{Introduction}

Language models that generate false information present two distinct failure modes. In the first,
the model produces factually incorrect claims without any intent to mislead---a \emph{hallucination}
arising from imprecise recall or distributional interpolation. In the second, the model deliberately
produces misleading outputs while concealing its true reasoning---\emph{deception} as studied in
the alignment faking literature \citep{greenblatt2024alignment}. These failure modes may share
surface symptoms (incorrect text) but arise from different mechanisms.

Recent work has proposed using internal model features as reward signals for reinforcement learning
(RL). Goodfire AI's \emph{Features as Rewards} (RLFR) framework \citep{goodfire2026rlfr} trains
models to produce outputs that activate truthfulness-associated features, achieving 58\% hallucination
reduction. This raises several questions we address in this work:

\begin{enumerate}
    \item Are truthfulness and deception geometrically distinct in activation space, or do they
          share the same underlying representation?
    \item Can linear probes for each be trained reliably, and how do they generalize?
    \item Do probe-based pipelines reduce hallucinations in practice, and where do they fail?
    \item Are these probes robust to adversarial fine-tuning and surface rephrasing?
\end{enumerate}

We conduct five controlled experiments on Gemma-2-2B-it \citep{gemma2024}, establishing a
reproducible benchmark for probe-based hallucination and deception detection.

\section{Background and Related Work}

\paragraph{Truthfulness probing.}
Linear probes on language model hidden states have been used to detect whether models
``know'' their output is false \citep{marks2023geometry,li2023inference}. These probes
exploit the finding that models maintain internal representations of truthfulness even when
generating incorrect outputs. \citet{truthfulqa} provide the canonical benchmark.

\paragraph{Alignment faking.}
\citet{greenblatt2024alignment} demonstrate that frontier models can engage in strategic
deception, maintaining aligned behavior under observation while pursuing misaligned goals
when unmonitored. The lambda\_results study (unpublished, 2026) finds that the alignment
faking signal forms a one-dimensional subspace in weight space (rank-1 LoRA suffices for
0.968 AUROC) and is 99.6\% orthogonal to monitoring-awareness features.

\paragraph{Features as Rewards.}
\citet{goodfire2026rlfr} propose using interpretability features---including probes for
truthfulness---as differentiable reward signals for RL. Their system achieves 58\% hallucination
reduction on factual generation tasks. Our work evaluates probe quality independently of the RL
training loop, identifying the limitations that any such approach must overcome.

\paragraph{Adversarial probe robustness.}
The obfuscation-probe-robustness study (2026) finds that adversarial supervised fine-tuning
(SFT) against deception probes can rotate the signal in activation space but cannot eliminate
it: fresh probes trained on adversarially-modified activations consistently recover to $\geq 0.897$
AUROC. We test whether this finding generalizes to truthfulness probes.

\section{Methods}

\subsection{Model and Hardware}

All experiments use \textbf{Gemma-2-2B-it} \citep{gemma2024}, a 2.6B parameter instruction-tuned
model with 26 transformer layers and 2304-dimensional hidden states. Experiments run on an
NVIDIA RTX 4070 Ti SUPER (16GB VRAM). All random seeds are fixed at 42.

\subsection{Probe Architecture}

We use \textbf{logistic regression} with L2 regularization (scikit-learn \texttt{LogisticRegression}).
Features are last-token hidden states at a fixed layer, normalized with \texttt{StandardScaler}.
Hyperparameter $C \in \{0.01, 0.1, 1.0, 10.0\}$ is selected by 5-fold stratified cross-validation.
AUROC is the primary metric.

\subsection{Experiment 1: Truthfulness Probe}

\paragraph{Data.} TruthfulQA generation split \citep{truthfulqa}, 817 questions. Each question
contributes one positive (correct answer) and one negative (first incorrect answer), formatted as
\texttt{"Q: \{question\}\textbackslash nA: \{answer\}"}, yielding 1634 paired samples. 80/20
train/test split, stratified by label.

\paragraph{Layer sweep.} We extract last-token hidden states at all 26 layers and train
independent probes, selecting the best layer by CV AUROC.

\paragraph{Ablations.} Random labels, shuffled activations, and bag-of-words (BoW, TF-IDF unigrams)
baselines verify that the signal is geometric rather than superficial.

\subsection{Experiment 2: Deception Probe}

\paragraph{Data.} 500 aligned scratchpad samples (honest class) from \texttt{vincentoh/alignment-faking-training}
and 500 deceptive samples (AF organisms with probe score $> 0.95$ and potential-faking outputs).
Held-out evaluation on \textbf{gold\_106}: 106 hand-labeled samples (51 AF, 55 aligned), never
used in training. Gold\_106 is the only human-labeled alignment faking evaluation set available.

\paragraph{Cross-model setup.} AF organisms were generated by Mistral-7B but activations are
extracted from Gemma-2-2B, testing whether the deception signal is model-universal.

\subsection{Experiment 3: Orthogonality Verification}

Let $\mathbf{w}_t \in \mathbb{R}^{2304}$ and $\mathbf{w}_d \in \mathbb{R}^{2304}$ be the probe
weight vectors from Experiments 1 and 2. We measure:

\begin{itemize}
    \item Cosine similarity: $\cos(\mathbf{w}_t, \mathbf{w}_d) = \frac{\mathbf{w}_t \cdot \mathbf{w}_d}{\|\mathbf{w}_t\| \|\mathbf{w}_d\|}$
    \item Principal angles between PCA subspaces of the respective training activations
    \item Cross-task AUROC: applying each probe to the other's classification task
    \item Combined 2D probe: $f(x) = \sigma(a_1 \cdot p_t(x) + a_2 \cdot p_d(x) + b)$ where
          $p_t, p_d$ are the scalar probe outputs
\end{itemize}

\subsection{Experiment 4: End-to-End Pipeline}

\paragraph{Generation.} We prompt Gemma-2-2B-it to generate 20 factual paragraphs across
5 categories: obscure scientists, historical dates, technical specifications, paper citations,
and biographical details. These categories are chosen to maximize hallucination rate.

\paragraph{Claim extraction.} Claude-sonnet-4-6 extracts atomic factual claims from each
paragraph as JSON, labeling each as \texttt{correct}, \texttt{incorrect}, or \texttt{unverifiable}.

\paragraph{Probe scoring.} The truthfulness probe scores each claim's activation.

\paragraph{Intervention.} Claims with probe score $P(\text{correct}) < 0.5$ are passed to
Claude-sonnet-4-6 for correction or retraction.

\paragraph{Baselines.} We compare against a random probe (random weight vector, same architecture)
and a Claude self-check baseline (Claude-sonnet-4-6 directly answering ``Is this claim factually correct?'').

\paragraph{Hallucination reduction rate.} (Correctly fixed + correctly retracted) / total
incorrect claims, following \citet{goodfire2026rlfr}.

\subsection{Experiment 5: Adversarial Robustness}

\paragraph{5.1 Probe evasion.} We fine-tune Gemma-2-2B-it with LoRA ($r=8$, $\alpha=16$,
$\text{lr}=2\times10^{-5}$, 500 steps) using an adversarial loss that minimizes the probe
score on truthful activations:
\[
    \mathcal{L}_{\text{adv}} = \mathbf{w}_t \cdot \frac{\mathbf{h}_{\ell}(x)}{\|\mathbf{h}_{\ell}(x)\|}
\]
where $\mathbf{h}_{\ell}(x)$ is the last-token hidden state at layer $\ell = 16$ for a
truthful input $x$. We then evaluate: (a) the original probe on adversarial model activations,
and (b) a fresh probe trained on those activations.

\paragraph{5.2 Paraphrase attack.} We take 19 incorrect claims from Experiment 4, generate
5 paraphrases each via Claude-sonnet-4-6, and measure probe score variance:
$\sigma^2_{\text{para}} = \text{Var}(p_t(\text{paraphrase}_i))$.
A low standard deviation ($< 0.10$) indicates the probe detects factual content rather than
surface wording.

\section{Results}

\subsection{Experiment 1: Truthfulness Probe}

Table~\ref{tab:exp1} shows final probe performance. The signal peaks at \textbf{layer 16}
with last-token pooling (CV AUROC 0.901), confirming that truthfulness representations
concentrate in the upper-middle layers and at sequence endpoints.

\begin{table}[h]
\centering
\caption{Truthfulness probe performance (Experiment 1).}
\label{tab:exp1}
\begin{tabular}{lc}
\toprule
Metric & Value \\
\midrule
AUROC & 0.877 [0.836, 0.910] \\
AUPRC & 0.887 \\
Accuracy & 0.774 \\
F1 & 0.774 \\
Brier Score & 0.153 \\
\midrule
\multicolumn{2}{l}{\textit{Ablations}} \\
Random labels & 0.505 \\
Shuffled activations & 0.475 \\
Bag-of-words baseline & \textbf{0.311} \\
\bottomrule
\end{tabular}
\end{table}

The BoW baseline achieves only 0.311 AUROC---well below the probe's 0.877---confirming the
signal is geometric rather than lexical. All ablations return near-chance performance,
validating probe integrity.

\paragraph{Pooling comparison at layer 16.}
Last-token (0.901) $\gg$ mean (0.767) $\gg$ first-token (0.513). The truthfulness signal
concentrates at the final generated token, consistent with the view that the model's
``decision'' about what to generate is encoded at the last position.

\subsection{Experiment 2: Deception Probe}

\begin{table}[h]
\centering
\caption{Deception probe performance (Experiment 2). BoW warning indicates vocabulary confound.}
\label{tab:exp2}
\begin{tabular}{lc}
\toprule
Eval Set & AUROC \\
\midrule
In-distribution test & 0.981 [0.959, 0.996] \\
gold\_106 (held-out OOD) & 0.663 [0.561, 0.764] \\
\midrule
\multicolumn{2}{l}{\textit{Ablations}} \\
Random labels & 0.486 \\
\textbf{Bag-of-words baseline} & \textbf{0.997} \\
No scratchpad tags & 0.997 \\
Truncated 128 tokens & 0.933 \\
\midrule
False positive rate (clean text) & 96.7\% \\
\bottomrule
\end{tabular}
\end{table}

The deception probe achieves 0.981 AUROC in-distribution but critically, the \textbf{BoW
baseline achieves 0.997}---matching or exceeding the probe. This indicates the probe has
learned surface vocabulary patterns from the AF scratchpad format rather than deeper
representational features of deceptive intent. Removing scratchpad tags does not change
the BoW performance (0.997), and the 0.663 AUROC on gold\_106 confirms weak OOD
generalization. The false positive rate of 96.7\% on clean factual text (virtually
everything classified as deceptive) further underscores the probe's reliance on
format-specific cues.

\subsection{Experiment 3: Orthogonality Verification}

\begin{table}[h]
\centering
\caption{Orthogonality between truthfulness and deception probe weight vectors.}
\label{tab:orthogonality}
\begin{tabular}{lcc}
\toprule
Metric & Value & Criterion \\
\midrule
$\cos(\mathbf{w}_t, \mathbf{w}_d)$ & $-0.0012$ & $\leq 0.15$ \checkmark \\
Angle & $89.9°$ & --- \\
Same-layer cos (layer 16) & $-0.0043$ & --- \\
Same-layer cos (layer 26) & $-0.0050$ & --- \\
PCA-1 subspace angle & $89.8°$ & --- \\
PCA-50 mean angle & $81.8°$ & --- \\
\midrule
\multicolumn{3}{l}{\textit{Combined probe (2D input)}} \\
Truth-only AUROC & 0.817 & --- \\
Deception-only AUROC & 0.796 & --- \\
Combined AUROC & \textbf{0.868} & --- \\
\bottomrule
\end{tabular}
\end{table}

The probe weight vectors are near-perfectly orthogonal regardless of which layer is used for
comparison, and the PCA subspace analysis confirms the finding holds across the top-50 principal
components (mean angle 81.8°, all $> 70°$). This replicates the lambda\_results finding of
99.6\% orthogonal subspaces in an independent experimental setup.

\paragraph{Cross-task transfer.}
Applying the deception probe to the truthfulness task yields 0.588 AUROC (near-chance),
confirming the probes encode distinct information. The reverse (truthfulness probe on
deception task) yields 0.741, suggesting some transfer via text-format similarity.

\paragraph{Combined probe.}
Concatenating the two scalar probe outputs as a 2D feature and training a meta-classifier
yields AUROC 0.868, outperforming either probe alone (truth: 0.817, deception: 0.796).
The meta-classifier weights the truthfulness feature more heavily (8.15 vs 4.60), reflecting
its stronger individual signal on the mixed-task test set.

\subsection{Experiment 4: End-to-End Pipeline}

\paragraph{Hallucination rates.}
Gemma-2-2B-it hallucinated on 54.2\% of verifiable claims overall, ranging from 46.3\%
(historical dates) to 70.2\% (biographical details). Paper citations, despite being
virtually unmemorizable precisely, had the lowest hallucination rate (51.1\%) because
the model hedges more explicitly on precise numerical claims.

\begin{table}[h]
\centering
\caption{Method comparison on 253 verifiable claims (Experiment 4, sonnet-4-6 judge).}
\label{tab:exp4_methods}
\begin{tabular}{lcccc}
\toprule
Method & AUROC & Accuracy & Recall & F1 \\
\midrule
Claude self-check (sonnet-4-6) & \textbf{0.986} & \textbf{0.889} & \textbf{1.000} & \textbf{0.883} \\
Truthfulness probe & 0.592 & 0.601 & 0.076 & 0.137 \\
Random probe & 0.509 & 0.538 & 0.547 & 0.498 \\
\bottomrule
\end{tabular}
\end{table}

The truthfulness probe achieves only 0.592 AUROC on free-form generation---a severe
domain shift from the TruthfulQA QA format used for training. The probe flags 95.7\%
of all claims (242/253), essentially acting as a uniform detector.

\begin{table}[h]
\centering
\caption{Intervention results: hallucination reduction via probe flagging + LLM correction.}
\label{tab:intervention}
\begin{tabular}{lc}
\toprule
Metric & Value \\
\midrule
Claims flagged by probe & 242 / 253 (95.7\%) \\
True positives fixed & 135 / 147 \\
False positives handled & 6 / 6 \\
Missed incorrect claims & 9 / 147 \\
\textbf{Hallucination reduction} & \textbf{91.8\%} \\
\midrule
Goodfire RLFR (with RL training) & 58\% \\
\bottomrule
\end{tabular}
\end{table}

Despite the probe's weak discrimination, the pipeline achieves 91.8\% hallucination
reduction---surpassing the Goodfire RL baseline by 33.8 percentage points. The mechanism
is simple: the probe's high recall (near-total flagging) ensures few incorrect claims
escape; Claude-sonnet-4-6's intervention then correctly distinguishes true from false
positives with high accuracy. The probe contributes coverage, not precision.

\paragraph{Judge sensitivity.}
Using Claude-haiku-4-5 as judge instead of Claude-sonnet-4-6 produced 87.5\% hallucination
reduction with substantially noisier ground truth labels (59 unverifiable vs.\ 18 with
sonnet-4-6, and only 88 incorrect claims identified vs.\ 147). The choice of judge model
materially affects measured hallucination rates and intervention quality.

\subsection{Experiment 5: Adversarial Robustness}

\paragraph{5.1 Probe evasion.}

\begin{table}[h]
\centering
\caption{Probe evasion results after adversarial SFT (500 steps, $\text{lr}=2\times10^{-5}$).}
\label{tab:evasion}
\begin{tabular}{lcc}
\toprule
Metric & Value & Criterion \\
\midrule
Baseline probe AUROC & 0.876 & --- \\
Original probe on adversarial model & 0.494 & --- \\
AUROC drop & 0.382 & --- \\
Fresh probe recovery & 0.696 & $\geq 0.90$ \xmark \\
\bottomrule
\end{tabular}
\end{table}

Adversarial fine-tuning successfully defeats the original probe (0.876 $\to$ 0.494). A
fresh probe trained on the adversarial model's activations recovers to only 0.696---below
the 0.90 criterion and well below the $\geq 0.897$ recovery observed in
obfuscation-probe-robustness for deception probes.

This divergence may reflect that (a) the truthfulness signal is more susceptible to
rotation than the deception signal, (b) 500 steps at $\text{lr}=2\times10^{-5}$ is
over-destructive for this probe, or (c) the truthfulness signal is not as
``mechanistically load-bearing'' as deception---i.e., the model can learn to generate
truthful-looking activations without the original truthfulness circuit.

\paragraph{5.2 Paraphrase attacks.}

Mean probe score standard deviation across 5 paraphrases of each incorrect claim:
$\bar{\sigma} = 0.088$, below the 0.10 criterion. The probe is robust to surface
rephrasing on average. However, per-claim pass rate is only 57.9\%, with maximum
score differences up to 0.635 on individual claims. The probe's average behavior is
stable; individual claim scoring is not.

\section{Discussion}

\subsection{Orthogonality Has Practical Implications}

The near-perfect orthogonality of the two probe directions ($\theta = 89.9°$) is not
merely a geometric curiosity. It implies that a model can simultaneously (a) generate
factually incorrect claims without any deceptive intent, and (b) engage in strategic
deception while generating factually correct outputs. Monitoring for one failure mode
cannot substitute for monitoring the other. This corroborates the lambda\_results
finding that ``monitoring-awareness is 99.6\% orthogonal to AF intent,'' and suggests
it is a general property of how language models encode these distinct behavioral modes.

The combined 2D probe (AUROC 0.868) outperforming either individual probe (0.817, 0.796)
provides direct empirical evidence that the two signals are additive and non-redundant.

\subsection{The Domain Shift Problem}

The truthfulness probe's degradation from 0.877 (TruthfulQA QA pairs) to 0.592
(free-form factual generation) illustrates a fundamental challenge for probe-based
reward signals. If the probe is trained on a distribution that does not match the
generation distribution, its reward signal is unreliable. The autointerp study (2026)
resolves an analogous OOD gap (0.355 $\to$ 0.991 AUROC) by identifying that the root
cause was activation magnitude mismatch (10$\times$ difference between regimes) rather
than vocabulary, and fixing it by diversifying training data across both regimes.

A domain-matched truthfulness probe---trained on probe-like generated text rather than
TruthfulQA---would likely show substantially higher AUROC in the Experiment 4 setting.

\subsection{High-Recall Probes as Pipeline Components}

Our 91.8\% hallucination reduction demonstrates a counterintuitive finding: a low-precision
probe can be highly effective in a pipeline when paired with a capable corrector. By flagging
95.7\% of all claims and passing them to an LLM, the probe trades precision for recall,
and the LLM compensates for the false positive rate. This exceeds the Goodfire RL
baseline (58\%) without any RL training.

However, this result is qualified. The 91.8\% figure depends on Claude-sonnet-4-6's ability
to correctly classify flagged claims---a capability that exists independently of the probe.
A pipeline using Claude self-check alone (without any probe) would likely achieve comparable
reduction by checking all claims. The probe's marginal contribution is to provide a structured
interface between the generator and the corrector, not genuine discriminative signal.

\subsection{The Deception Probe's Vocabulary Confound}

The BoW baseline matching the deception probe (0.997 vs.\ 0.981) is a critical finding.
It indicates the probe learned to detect the distinctive vocabulary of AF scratchpad reasoning
rather than geometric features of deceptive intent. This is directly analogous to the
vocabulary confound documented in the autointerp SAE probe work, and explains the OOD gap
(0.663 on gold\_106, which uses different surface forms).

The practical implication: probes trained on scratchpad-formatted data cannot be deployed
as general-purpose deception detectors without substantial training data diversification.
The autointerp approach of training across both ``HIGH'' and ``LOW'' activation regimes,
combined with vocabulary-constrained negative sampling, is the likely path to closing
this gap.

\subsection{Adversarial Robustness: Truthfulness vs.\ Deception}

The truthfulness probe shows substantially weaker adversarial robustness than the deception
probe. Where the obfuscation-probe-robustness study found fresh deception probes consistently
recover to $\geq 0.897$ AUROC, our truthfulness probe only recovered to 0.696. This
asymmetry suggests that deception, as a behavioral mode, is more deeply entangled with
model function than truthfulness---consistent with the obfuscation-probe-robustness
finding that ``the deception signal is mechanistically load-bearing: can be rotated in
activation space but never eliminated.'' Whether the same holds for truthfulness at lower
adversarial intensity (fewer steps, lower learning rate) remains an open question.

\section{Limitations}

\paragraph{Single model.} All experiments use Gemma-2-2B-it. Larger models, different
architectures, and different instruction-tuning recipes may exhibit different probe geometry.

\paragraph{Claude-as-judge.} Ground truth labels in Experiment 4 come from Claude-sonnet-4-6,
not human annotators. This introduces potential systematic errors and makes the pipeline
partially circular: Claude corrects what Claude flagged as incorrect.

\paragraph{No RL evaluation.} We evaluate probes as discriminative classifiers, not as
reward signals in an RL training loop. The RLFR paper's results may depend on RL
dynamics---exploration, credit assignment, distributional shift during training---that
we do not capture.

\paragraph{AF organism provenance.} Deception training data comes from Mistral-7B organisms.
Whether Gemma-2-2B generates alignment faking in the same representational style is unknown.
The cross-model transfer test (Mistral organisms $\to$ Gemma activations $\to$ 0.981 AUROC)
suggests some universality, but the vocabulary confound makes this hard to interpret cleanly.

\section{Conclusion}

We establish that truthfulness and deception are orthogonal signals in Gemma-2-2B-it
activation space ($\cos = -0.0012$), replicating a key finding from the lambda\_results
mechanistic study. A truthfulness probe achieves 0.877 AUROC on TruthfulQA but suffers
severe domain shift to free-form generation (0.592 AUROC). A deception probe achieves
0.981 AUROC in-distribution but is vocabulary-confounded (BoW = 0.997) and fails OOD
(0.663 on gold\_106). Despite these limitations, the combined pipeline achieves 91.8\%
hallucination reduction on hard factual generation---exceeding the Goodfire RL baseline
by 33.8 points---by using the probe as a high-recall flag and delegating correction to a
capable LLM. The probe contributes coverage; the LLM contributes precision.

These results suggest two priorities for future work. First, domain-matched training
data is essential for probes to provide genuine reward signal---the autointerp approach
of matching activation magnitude regimes across training examples is a promising direction.
Second, the asymmetry between truthfulness and deception probe robustness (recovery 0.696
vs.\ $\geq 0.897$) warrants investigation into whether truthfulness, unlike deception,
is not mechanistically load-bearing in the model's generation process.

\bibliographystyle{plainnat}
\bibliography{refs}

\appendix

\section{Layer Sweep Results (Experiment 1)}

\begin{table}[h]
\centering
\small
\caption{Full layer sweep for truthfulness probe (5-fold CV AUROC, last-token pooling).}
\label{tab:layer_sweep}
\begin{tabular}{ccc|ccc}
\toprule
Layer & AUROC & Best C & Layer & AUROC & Best C \\
\midrule
0 & 0.529 & 0.01 & 14 & 0.875 & 0.01 \\
1 & 0.566 & 0.01 & \textbf{15} & \textbf{0.882} & \textbf{0.01} \\
2 & 0.580 & 1.0  & \textbf{16} & \textbf{0.901} & \textbf{0.01} \\
3 & 0.593 & 0.01 & 17 & 0.890 & 0.01 \\
4 & 0.604 & 0.01 & 18 & 0.885 & 0.01 \\
5 & 0.659 & 0.01 & 19 & 0.881 & 0.01 \\
6 & 0.711 & 0.01 & 20 & 0.875 & 0.01 \\
7 & 0.735 & 0.01 & 21 & 0.869 & 0.01 \\
8 & 0.752 & 0.01 & 22 & 0.862 & 0.01 \\
9 & 0.768 & 0.01 & 23 & 0.858 & 0.01 \\
10 & 0.778 & 0.01 & 24 & 0.855 & 0.01 \\
11 & 0.834 & 0.01 & 25 & 0.851 & 0.01 \\
12 & 0.843 & 0.01 & 26 & 0.842 & 0.01 \\
13 & 0.849 & 0.01 & & & \\
\bottomrule
\end{tabular}
\end{table}

\section{Experiment 4: Hallucination by Category}

\begin{table}[h]
\centering
\caption{Hallucination rates by category (Experiment 4, sonnet-4-6 judge).}
\label{tab:halluc_by_cat}
\begin{tabular}{lcccc}
\toprule
Category & Claims & Correct & Incorrect & Hallucination Rate \\
\midrule
biographical & 59 & 17 & 40 & 70.2\% \\
obscure\_scientist & 44 & 12 & 22 & 64.7\% \\
technical\_specs & 62 & 25 & 36 & 59.0\% \\
paper\_citations & 50 & 23 & 24 & 51.1\% \\
historical\_dates & 56 & 29 & 25 & 46.3\% \\
\midrule
\textbf{Overall} & \textbf{271} & \textbf{106} & \textbf{147} & \textbf{54.2\%} \\
\bottomrule
\end{tabular}
\end{table}

\end{document}
